{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing ViT Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from vit_rollout import VITAttentionRollout,rollout\n",
    "    from vit_explain import show_mask_on_image\n",
    "except ModuleNotFoundError as s:\n",
    "    print('Installing required files...')\n",
    "    url =f\"https://raw.githubusercontent.com/lilloukas/vit-explain/main/vit_rollout.py\"\n",
    "    url2 = f\"https://raw.githubusercontent.com/jacobgil/vit-explain/main/vit_explain.py\"\n",
    "    url3 = f\"https://raw.githubusercontent.com/jacobgil/vit-explain/main/vit_grad_rollout.py\"\n",
    "    urls = url,url2,url3\n",
    "    for url in urls:\n",
    "        !wget --no-cache --backups=1 {url}\n",
    "    from vit_rollout import VITAttentionRollout,rollout\n",
    "    from vit_explain import show_mask_on_image\n",
    "try:\n",
    "    import timm\n",
    "except:\n",
    "    !pip install timm\n",
    "    import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "colab = False\n",
    "\n",
    "# IMGS_PATH is the location of the images to be evaluated\n",
    "if colab:\n",
    "    MODEL_PATH ='/content/gdrive/Shareddrives/520 Project/Saved Models/ViT/best_ViT_one_layer.pth'\n",
    "else:\n",
    "    MODEL_PATH ='/projectnb2/dl523/projects/Sarcasm/520 Project/Saved_Models/best_ViT_one_layer.pth'\n",
    "    IMGS_PATH = '/projectnb/dl523/students/colejh/520/wikipaintings_small/wikipaintings_test' \n",
    "\n",
    "# basic pre-processing tasks for proper ViT data ingestion\n",
    "our_ViT = timm.create_model('vit_huge_patch14_224_in21k', pretrained = True, num_classes = 25)\n",
    "our_ViT.load_state_dict(torch.load(MODEL_PATH))\n",
    "config = resolve_data_config({}, model=our_ViT)\n",
    "transform = create_transform(**config)\n",
    "\n",
    "# Putting model on GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "our_ViT.to(device)\n",
    "our_ViT.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = '/projectnb2/dl523/projects/Sarcasm/520 Project/0x0.jpg'\n",
    "def visualize_attention(model,transform,img_path,img_size = 224,show_image = False,return_both = False,head_fusion = 'max'):\n",
    "    '''\n",
    "    Visualize the attention for a given image. Overlays the heatmap produced by every attention head in Vision Transformer\n",
    "    Returns the predicted art style, the heatmap image, and if desired, the original input image\n",
    "    Model: vision transformer model\n",
    "    transform: transform's applied to images which are input to the model\n",
    "    img_size: dependent on the vision transformer architechture \n",
    "    show_image: plot the heatmap image \n",
    "    return_both: if true, returns the heatmap image and the original image \n",
    "    head_fusion: describes how the attention head values are calculated: options are 'mean','max', and 'min'\n",
    "    \n",
    "    '''\n",
    "    img = Image.open(img_path)\n",
    "    test = transform(img).unsqueeze(0).to(device)\n",
    "    attention_rollout = VITAttentionRollout(model, head_fusion = head_fusion)\n",
    "    output,mask = attention_rollout(test)\n",
    "\n",
    "    img = img.resize((224, 224))\n",
    "    np_img = np.array(img)[:,:,::-1]\n",
    "    np_img = cv2.cvtColor(np_img,cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.resize(mask,(np_img.shape[1],np_img.shape[0]))\n",
    "    result = show_mask_on_image(np_img,mask)\n",
    "    rgb_img = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
    "    if show_image:\n",
    "        plt.imshow(rgb_img)\n",
    "        plt.show()\n",
    "    if return_both:\n",
    "        return output,rgb_img,np_img\n",
    "    return output,rgb_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dynamically adjusting the layout of the image grid\n",
    "def largest_factor_pair(dim):\n",
    "    '''\n",
    "    Returns the largest factor pair for the input value\n",
    "    '''\n",
    "    factor_pairs = []\n",
    "    for i in range(1, int(math.sqrt(dim))+1):\n",
    "        if dim % i == 0:\n",
    "            factor_pairs.append((i, dim / i))\n",
    "    return factor_pairs[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_CHOICES = True # For picking a random image from every art style to visualize attention\n",
    "styles = [ style for style in os.listdir(IMGS_PATH) if os.path.isdir(os.path.join(IMGS_PATH, style)) ]\n",
    "styles = sorted(styles)\n",
    "\n",
    "# Storing all the information for each image\n",
    "attentions = []\n",
    "originals = []\n",
    "original_style = []\n",
    "attention_style = []\n",
    "\n",
    "# If RANDOM_CHOICES is false, will show image grid of attentions for only the selected styles\n",
    "selected_styles = [random.choice(styles),random.choice(styles)]\n",
    "if RANDOM_CHOICES:\n",
    "    for style in styles:\n",
    "        style_path = os.path.join(IMGS_PATH, style)\n",
    "        os.chdir(style_path)\n",
    "        images = glob.glob(\"*.jpg\")\n",
    "        img = random.choice(images)\n",
    "        output,att,orig = visualize_attention(our_ViT,transform,style_path+'/'+img,return_both = True,head_fusion = 'max')\n",
    "        attentions.append(att)\n",
    "        originals.append(orig)\n",
    "        original_style.append(style)\n",
    "        predicted = torch.argmax(output,1).cpu().item()\n",
    "        attention_style.append(styles[predicted])\n",
    "\n",
    "else:\n",
    "    for style in styles:\n",
    "        if style in selected_styles:\n",
    "            style_path = os.path.join(IMGS_PATH, style)\n",
    "            os.chdir(style_path)\n",
    "\n",
    "            # List of all images in given style directory\n",
    "            images = glob.glob(\"*.jpg\") \n",
    "            for img in images:\n",
    "                output,att,orig = visualize_attention(our_ViT,transform,style_path+'/'+img,return_both = True,head_fusion = 'max')\n",
    "                attentions.append(att)\n",
    "                originals.append(orig)\n",
    "                original_style.append(style)\n",
    "                predicted = torch.argmax(output,1).cpu().item()\n",
    "                attention_style.append(styles[predicted])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots a grid of attention images and their corresponding original image\n",
    "if originals:\n",
    "    if RANDOM_CHOICES:\n",
    "        imgs = []\n",
    "        labels =[]\n",
    "        for first,pred,second,true in zip(attentions,attention_style,originals,original_style):\n",
    "            imgs.append(first)\n",
    "            labels.append(pred)\n",
    "            imgs.append(second)\n",
    "            labels.append(true)\n",
    "        fig = plt.figure(figsize=(35., 35.))\n",
    "        grid = ImageGrid(fig, 111, \n",
    "                         nrows_ncols=(5,10),  \n",
    "                         axes_pad=0.3, \n",
    "                         )\n",
    "        for ax, im,lab in zip(grid, imgs,labels):\n",
    "\n",
    "            ax.imshow(im)\n",
    "            ax.set_title(lab)\n",
    "        plt.show()\n",
    "    else:\n",
    "        imgs = []\n",
    "        labels =[]\n",
    "        for first,pred,second,true in zip(attentions,attention_style,originals,original_style):\n",
    "            imgs.append(first)\n",
    "            labels.append(pred)\n",
    "            imgs.append(second)\n",
    "            labels.append(true)\n",
    "        fig = plt.figure(figsize=(25., 25.))\n",
    "        dim = largest_factor_pair(len(imgs))\n",
    "        grid = ImageGrid(fig, 111,  \n",
    "                         nrows_ncols=(int(dim[0]), int(dim[1])), \n",
    "                         axes_pad=0.3, \n",
    "                         )\n",
    "        for ax, im,lab in zip(grid, imgs,labels):\n",
    "\n",
    "            ax.imshow(im)\n",
    "            ax.set_title(lab)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
