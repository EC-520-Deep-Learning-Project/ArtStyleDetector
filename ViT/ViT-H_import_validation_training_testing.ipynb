{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c30fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr4/ec504/kjv/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr4/ec504/kjv/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/usr4/ec504/kjv/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr4/ec504/kjv/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from PIL import Image\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d500add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for training (fine-tuning) and testing model\n",
    "        \n",
    "def finetune_ViT(model, trainloader, optimizer, criterion, num_epochs, scheduler=None):\n",
    "    \n",
    "    m = nn.Sigmoid()\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        running_loss = 0\n",
    "        for i, data in enumerate(trainloader, start=0):\n",
    "            \n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            outputs = m(outputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 20 == 19:    # print every 20 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 20))\n",
    "                running_loss = 0.0\n",
    "         \n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "            \n",
    "\n",
    "def test_ViT(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    m = nn.Sigmoid()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            \n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            #print(\"True values: \", labels)\n",
    "            \n",
    "            outputs = our_ViT(images)\n",
    "            probabilities = m(outputs)\n",
    "            #print(\"Probabilities: \", probabilities)\n",
    "            \n",
    "            predicted = torch.argmax(outputs,1)\n",
    "            #print(\"Predicted: \", predicted)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #print(\"Correct: \", correct)\n",
    "        \n",
    "        acc = correct/total * 100\n",
    "        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6824a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define validation testing function for determining optimal learning rates and batch sizes\n",
    "\n",
    "def perform_validation_testing(model, L, validationset, lr_array, batchsize_array, criterion, num_epochs=5, scheduler=None):\n",
    "\n",
    "    # create stratified folds to preserve class percentage representation within folds\n",
    "    skf = StratifiedKFold(n_splits = L, shuffle = True, random_state = 1)\n",
    "\n",
    "    # split validationset tuples into data and labels\n",
    "    val_data, val_labels = zip(*validationset)\n",
    "\n",
    "    accuracies = np.empty([len(lr_array), len(batchsize_array)])\n",
    "\n",
    "    # run validation testing\n",
    "\n",
    "    for i, lr in enumerate(lr_array):\n",
    "        \n",
    "        optimizer = optim.SGD(filter(lambda layer: layer.requires_grad, model.parameters()), lr=lr, momentum = .9)\n",
    "\n",
    "        for j, batch_size in enumerate(batchsize_array):\n",
    "\n",
    "            fold_val_acc = []\n",
    "            print(\"LEARNING RATE: \", lr)\n",
    "            print(\"BATCH SIZE: \", batch_size)\n",
    "\n",
    "            for fold, (train_ids, test_ids) in enumerate(skf.split(val_data, val_labels)):\n",
    "\n",
    "                # restore original weights for ViT and reset optimizer\n",
    "                model.load_state_dict(torch.load(\"/projectnb/dl523/students/kjv/EC520_Project/ViT/Saved_Models/untrained_ViT.pth\"))\n",
    "\n",
    "                print(\"---STARTING NEW FOLD---\")\n",
    "\n",
    "                trainfold_sampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "                testfold_sampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "                # define data loaders for training and testing data in this fold\n",
    "                val_trainloader = torch.utils.data.DataLoader(validationset, batch_size=batch_size, sampler=trainfold_sampler)\n",
    "                val_testloader = torch.utils.data.DataLoader(validationset, batch_size=batch_size, sampler=testfold_sampler)\n",
    "\n",
    "                # train ViT on L-1 folds\n",
    "                finetune_ViT(model, trainloader=val_trainloader, optimizer=optimizer, criterion=criterion, num_epochs=num_epochs, scheduler=scheduler)\n",
    "\n",
    "                # test validation accuracy on remaining fold and keep track of accuracy per fold\n",
    "                result = test_ViT(model, val_testloader)\n",
    "                fold_val_acc.append(result)\n",
    "\n",
    "            # take average accuracy across all folds for given learning rate\n",
    "            print(\"---ALL FOLD ACCURACIES FOR CURRENT LR/BATCH_SIZE---\")\n",
    "            print(fold_val_acc)\n",
    "            accuracies[i,j] = sum(fold_val_acc)/len(fold_val_acc)\n",
    "            print(f\"Average = {accuracies[i,j]}\\n\")\n",
    "\n",
    "\n",
    "    # choose learning rate with best validation accuracy\n",
    "    print(\"---FINAL AVG ACCURACIES PER LEARNING RATE/BATCH SIZE COMBO---\")\n",
    "    print(\"Accuracy Matrix: \\n\", accuracies)\n",
    "    best_lr_ind, best_bs_ind = np.unravel_index(np.argmax(accuracies, axis=None), accuracies.shape)\n",
    "    \n",
    "    optimal_lr = learning_rates[best_lr_ind]\n",
    "    optimal_batch_size = batchsize_array[best_bs_ind]\n",
    "\n",
    "    print(f\"\\nBest learning rate: {optimal_lr}\")\n",
    "    print(f\"Best batch size: {optimal_batch_size}\")\n",
    "    return optimal_lr, optimal_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3cc59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning helper methods\n",
    "\n",
    "def freeze_layers(model):\n",
    "    \"\"\"\n",
    "    Freeze all model layers\n",
    "    \"\"\"\n",
    "    for param in our_ViT.parameters():\n",
    "        param.requires_grad = False\n",
    "            \n",
    "def unfreeze_layers(model):\n",
    "    \"\"\"\n",
    "    Reset requires_grad for all params\n",
    "    \"\"\"\n",
    "    for param in our_ViT.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "051d9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for function of finetuning variable num of layers\n",
    "#layer_names = []\n",
    "\n",
    "#for name, _ in our_ViT.named_modules():\n",
    "#    layer_names.append(name)\n",
    "    \n",
    "#layers_to_freeze = layer_names[:-2-1]\n",
    "\n",
    "#print(layers_to_freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e13e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- timm Implemenation of ViT -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4021e87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://storage.googleapis.com/vit_models/imagenet21k/ViT-H_14.npz',\n",
       " 'num_classes': 21843,\n",
       " 'input_size': (3, 224, 224),\n",
       " 'pool_size': None,\n",
       " 'crop_pct': 0.9,\n",
       " 'interpolation': 'bicubic',\n",
       " 'fixed_input_size': True,\n",
       " 'mean': (0.5, 0.5, 0.5),\n",
       " 'std': (0.5, 0.5, 0.5),\n",
       " 'first_conv': 'patch_embed.proj',\n",
       " 'classifier': 'head',\n",
       " 'hf_hub': 'timm/vit_huge_patch14_224_in21k',\n",
       " 'architecture': 'vit_huge_patch14_224_in21k'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create default model if needed\n",
    "ViT = timm.create_model('vit_huge_patch14_224_in21k', pretrained=True)\n",
    "\n",
    "# output the config info for the default pre-trained model\n",
    "ViT.default_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc55ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing representation layer for fine-tuning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1280, out_features=25, bias=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create ViT for art style classification\n",
    "our_ViT = timm.create_model('vit_huge_patch14_224_in21k', pretrained = True, num_classes = 25)\n",
    "\n",
    "# confirm changes in classifier output\n",
    "our_ViT.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f896f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic pre-processing tasks for proper ViT data ingestion\n",
    "\n",
    "config = resolve_data_config({}, model=our_ViT)\n",
    "transform = create_transform(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2e9703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head.weight\n",
      "head.bias\n"
     ]
    }
   ],
   "source": [
    "# pre-training model setup\n",
    "\n",
    "# freeze all layers of the ViT\n",
    "freeze_layers(our_ViT)\n",
    "\n",
    "# unfreeze desired layers for fine-tuning\n",
    "our_ViT.head.bias.requires_grad = True\n",
    "our_ViT.head.weight.requires_grad = True\n",
    "\n",
    "# check that correct layers are frozen\n",
    "for name, param in our_ViT.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0daedcbb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (12): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (13): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (14): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (15): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (16): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (17): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (18): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (19): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (20): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (21): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (22): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (23): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (24): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (25): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (26): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (27): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (28): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (29): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (30): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (31): Block(\n",
       "      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Linear(in_features=1280, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move model to GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# save starting state for ViT\n",
    "torch.save(our_ViT.state_dict(), \"/projectnb/dl523/students/kjv/EC520_Project/ViT/Saved_Models/untrained_ViT.pth\")\n",
    "\n",
    "our_ViT.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9232472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize validation dataset\n",
    "\n",
    "validationpath = \"/projectnb/dl523/students/kjv/EC520_Project/Data/wikipaintings_full/wikipaintings_val\"\n",
    "validationset = datasets.ImageFolder(validationpath, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d66848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE:  0.003\n",
      "BATCH SIZE:  32\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.218\n",
      "[1,    40] loss: 3.214\n",
      "[1,    60] loss: 3.210\n",
      "[1,    80] loss: 3.205\n",
      "[1,   100] loss: 3.202\n",
      "[1,   120] loss: 3.199\n",
      "[1,   140] loss: 3.196\n",
      "[1,   160] loss: 3.191\n",
      "[1,   180] loss: 3.186\n",
      "[2,    20] loss: 3.182\n",
      "[2,    40] loss: 3.179\n",
      "[2,    60] loss: 3.175\n",
      "[2,    80] loss: 3.171\n",
      "[2,   100] loss: 3.166\n",
      "[2,   120] loss: 3.164\n",
      "[2,   140] loss: 3.162\n",
      "[2,   160] loss: 3.162\n",
      "[2,   180] loss: 3.148\n",
      "[3,    20] loss: 3.153\n",
      "[3,    40] loss: 3.152\n",
      "[3,    60] loss: 3.145\n",
      "[3,    80] loss: 3.139\n",
      "[3,   100] loss: 3.132\n",
      "[3,   120] loss: 3.135\n",
      "[3,   140] loss: 3.137\n",
      "[3,   160] loss: 3.130\n",
      "[3,   180] loss: 3.131\n",
      "[4,    20] loss: 3.127\n",
      "[4,    40] loss: 3.126\n",
      "[4,    60] loss: 3.124\n",
      "[4,    80] loss: 3.105\n",
      "[4,   100] loss: 3.124\n",
      "[4,   120] loss: 3.108\n",
      "[4,   140] loss: 3.114\n",
      "[4,   160] loss: 3.112\n",
      "[4,   180] loss: 3.104\n",
      "[5,    20] loss: 3.107\n",
      "[5,    40] loss: 3.101\n",
      "[5,    60] loss: 3.104\n",
      "[5,    80] loss: 3.097\n",
      "[5,   100] loss: 3.100\n",
      "[5,   120] loss: 3.094\n",
      "[5,   140] loss: 3.091\n",
      "[5,   160] loss: 3.086\n",
      "[5,   180] loss: 3.090\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[1,    40] loss: 3.213\n",
      "[1,    60] loss: 3.210\n",
      "[1,    80] loss: 3.206\n",
      "[1,   100] loss: 3.200\n",
      "[1,   120] loss: 3.197\n",
      "[1,   140] loss: 3.194\n",
      "[1,   160] loss: 3.188\n",
      "[1,   180] loss: 3.187\n",
      "[2,    20] loss: 3.179\n",
      "[2,    40] loss: 3.178\n",
      "[2,    60] loss: 3.173\n",
      "[2,    80] loss: 3.167\n",
      "[2,   100] loss: 3.168\n",
      "[2,   120] loss: 3.161\n",
      "[2,   140] loss: 3.164\n",
      "[2,   160] loss: 3.156\n",
      "[2,   180] loss: 3.154\n",
      "[3,    20] loss: 3.146\n",
      "[3,    40] loss: 3.144\n",
      "[3,    60] loss: 3.143\n",
      "[3,    80] loss: 3.136\n",
      "[3,   100] loss: 3.137\n",
      "[3,   120] loss: 3.137\n",
      "[3,   140] loss: 3.140\n",
      "[3,   160] loss: 3.129\n",
      "[3,   180] loss: 3.127\n",
      "[4,    20] loss: 3.124\n",
      "[4,    40] loss: 3.124\n",
      "[4,    60] loss: 3.110\n",
      "[4,    80] loss: 3.119\n",
      "[4,   100] loss: 3.123\n",
      "[4,   120] loss: 3.109\n",
      "[4,   140] loss: 3.105\n",
      "[4,   160] loss: 3.108\n",
      "[4,   180] loss: 3.108\n",
      "[5,    20] loss: 3.098\n",
      "[5,    40] loss: 3.107\n",
      "[5,    60] loss: 3.091\n",
      "[5,    80] loss: 3.094\n",
      "[5,   100] loss: 3.106\n",
      "[5,   120] loss: 3.094\n",
      "[5,   140] loss: 3.083\n",
      "[5,   160] loss: 3.090\n",
      "[5,   180] loss: 3.095\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.218\n",
      "[1,    40] loss: 3.215\n",
      "[1,    60] loss: 3.209\n",
      "[1,    80] loss: 3.205\n",
      "[1,   100] loss: 3.200\n",
      "[1,   120] loss: 3.198\n",
      "[1,   140] loss: 3.196\n",
      "[1,   160] loss: 3.187\n",
      "[1,   180] loss: 3.185\n",
      "[2,    20] loss: 3.182\n",
      "[2,    40] loss: 3.175\n",
      "[2,    60] loss: 3.174\n",
      "[2,    80] loss: 3.172\n",
      "[2,   100] loss: 3.172\n",
      "[2,   120] loss: 3.165\n",
      "[2,   140] loss: 3.160\n",
      "[2,   160] loss: 3.158\n",
      "[2,   180] loss: 3.151\n",
      "[3,    20] loss: 3.154\n",
      "[3,    40] loss: 3.148\n",
      "[3,    60] loss: 3.139\n",
      "[3,    80] loss: 3.145\n",
      "[3,   100] loss: 3.135\n",
      "[3,   120] loss: 3.138\n",
      "[3,   140] loss: 3.128\n",
      "[3,   160] loss: 3.127\n",
      "[3,   180] loss: 3.132\n",
      "[4,    20] loss: 3.126\n",
      "[4,    40] loss: 3.111\n",
      "[4,    60] loss: 3.122\n",
      "[4,    80] loss: 3.119\n",
      "[4,   100] loss: 3.119\n",
      "[4,   120] loss: 3.115\n",
      "[4,   140] loss: 3.104\n",
      "[4,   160] loss: 3.114\n",
      "[4,   180] loss: 3.108\n",
      "[5,    20] loss: 3.108\n",
      "[5,    40] loss: 3.105\n",
      "[5,    60] loss: 3.099\n",
      "[5,    80] loss: 3.091\n",
      "[5,   100] loss: 3.103\n",
      "[5,   120] loss: 3.095\n",
      "[5,   140] loss: 3.094\n",
      "[5,   160] loss: 3.084\n",
      "[5,   180] loss: 3.096\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.216\n",
      "[1,    40] loss: 3.215\n",
      "[1,    60] loss: 3.209\n",
      "[1,    80] loss: 3.206\n",
      "[1,   100] loss: 3.201\n",
      "[1,   120] loss: 3.196\n",
      "[1,   140] loss: 3.193\n",
      "[1,   160] loss: 3.191\n",
      "[1,   180] loss: 3.187\n",
      "[2,    20] loss: 3.182\n",
      "[2,    40] loss: 3.176\n",
      "[2,    60] loss: 3.170\n",
      "[2,    80] loss: 3.174\n",
      "[2,   100] loss: 3.165\n",
      "[2,   120] loss: 3.166\n",
      "[2,   140] loss: 3.156\n",
      "[2,   160] loss: 3.159\n",
      "[2,   180] loss: 3.151\n",
      "[3,    20] loss: 3.150\n",
      "[3,    40] loss: 3.148\n",
      "[3,    60] loss: 3.138\n",
      "[3,    80] loss: 3.144\n",
      "[3,   100] loss: 3.139\n",
      "[3,   120] loss: 3.135\n",
      "[3,   140] loss: 3.130\n",
      "[3,   160] loss: 3.132\n",
      "[3,   180] loss: 3.123\n",
      "[4,    20] loss: 3.128\n",
      "[4,    40] loss: 3.121\n",
      "[4,    60] loss: 3.121\n",
      "[4,    80] loss: 3.118\n",
      "[4,   100] loss: 3.112\n",
      "[4,   120] loss: 3.115\n",
      "[4,   140] loss: 3.111\n",
      "[4,   160] loss: 3.100\n",
      "[4,   180] loss: 3.109\n",
      "[5,    20] loss: 3.107\n",
      "[5,    40] loss: 3.094\n",
      "[5,    60] loss: 3.101\n",
      "[5,    80] loss: 3.099\n",
      "[5,   100] loss: 3.097\n",
      "[5,   120] loss: 3.097\n",
      "[5,   140] loss: 3.098\n",
      "[5,   160] loss: 3.088\n",
      "[5,   180] loss: 3.085\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[1,    40] loss: 3.213\n",
      "[1,    60] loss: 3.210\n",
      "[1,    80] loss: 3.206\n",
      "[1,   100] loss: 3.200\n",
      "[1,   120] loss: 3.198\n",
      "[1,   140] loss: 3.194\n",
      "[1,   160] loss: 3.190\n",
      "[1,   180] loss: 3.182\n",
      "[2,    20] loss: 3.180\n",
      "[2,    40] loss: 3.178\n",
      "[2,    60] loss: 3.171\n",
      "[2,    80] loss: 3.171\n",
      "[2,   100] loss: 3.169\n",
      "[2,   120] loss: 3.160\n",
      "[2,   140] loss: 3.162\n",
      "[2,   160] loss: 3.155\n",
      "[2,   180] loss: 3.155\n",
      "[3,    20] loss: 3.148\n",
      "[3,    40] loss: 3.150\n",
      "[3,    60] loss: 3.140\n",
      "[3,    80] loss: 3.138\n",
      "[3,   100] loss: 3.142\n",
      "[3,   120] loss: 3.132\n",
      "[3,   140] loss: 3.135\n",
      "[3,   160] loss: 3.126\n",
      "[3,   180] loss: 3.132\n",
      "[4,    20] loss: 3.126\n",
      "[4,    40] loss: 3.127\n",
      "[4,    60] loss: 3.115\n",
      "[4,    80] loss: 3.119\n",
      "[4,   100] loss: 3.123\n",
      "[4,   120] loss: 3.106\n",
      "[4,   140] loss: 3.103\n",
      "[4,   160] loss: 3.109\n",
      "[4,   180] loss: 3.105\n",
      "[5,    20] loss: 3.110\n",
      "[5,    40] loss: 3.107\n",
      "[5,    60] loss: 3.098\n",
      "[5,    80] loss: 3.094\n",
      "[5,   100] loss: 3.089\n",
      "[5,   120] loss: 3.092\n",
      "[5,   140] loss: 3.092\n",
      "[5,   160] loss: 3.095\n",
      "[5,   180] loss: 3.084\n",
      "---ALL FOLD ACCURACIES FOR CURRENT LR/BATCH_SIZE---\n",
      "[19.16046039268788, 18.88964116452268, 18.348002708192283, 18.089430894308943, 18.56368563685637]\n",
      "Average = 18.61024415931363\n",
      "\n",
      "LEARNING RATE:  0.003\n",
      "BATCH SIZE:  64\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.218\n",
      "[1,    40] loss: 3.213\n",
      "[1,    60] loss: 3.210\n",
      "[1,    80] loss: 3.205\n",
      "[2,    20] loss: 3.198\n",
      "[2,    40] loss: 3.193\n",
      "[2,    60] loss: 3.190\n",
      "[2,    80] loss: 3.188\n",
      "[3,    20] loss: 3.180\n",
      "[3,    40] loss: 3.177\n",
      "[3,    60] loss: 3.175\n",
      "[3,    80] loss: 3.169\n",
      "[4,    20] loss: 3.164\n",
      "[4,    40] loss: 3.159\n",
      "[4,    60] loss: 3.157\n",
      "[4,    80] loss: 3.157\n",
      "[5,    20] loss: 3.145\n",
      "[5,    40] loss: 3.149\n",
      "[5,    60] loss: 3.146\n",
      "[5,    80] loss: 3.138\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[1,    40] loss: 3.213\n",
      "[1,    60] loss: 3.208\n",
      "[1,    80] loss: 3.205\n",
      "[2,    20] loss: 3.198\n",
      "[2,    40] loss: 3.194\n",
      "[2,    60] loss: 3.191\n",
      "[2,    80] loss: 3.185\n",
      "[3,    20] loss: 3.182\n",
      "[3,    40] loss: 3.177\n",
      "[3,    60] loss: 3.170\n",
      "[3,    80] loss: 3.166\n",
      "[4,    20] loss: 3.166\n",
      "[4,    40] loss: 3.160\n",
      "[4,    60] loss: 3.157\n",
      "[4,    80] loss: 3.153\n",
      "[5,    20] loss: 3.150\n",
      "[5,    40] loss: 3.147\n",
      "[5,    60] loss: 3.144\n",
      "[5,    80] loss: 3.136\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.218\n",
      "[1,    40] loss: 3.213\n",
      "[1,    60] loss: 3.208\n",
      "[1,    80] loss: 3.205\n",
      "[2,    20] loss: 3.200\n",
      "[2,    40] loss: 3.194\n",
      "[2,    60] loss: 3.190\n",
      "[2,    80] loss: 3.187\n",
      "[3,    20] loss: 3.180\n",
      "[3,    40] loss: 3.176\n",
      "[3,    60] loss: 3.172\n",
      "[3,    80] loss: 3.172\n",
      "[4,    20] loss: 3.163\n",
      "[4,    40] loss: 3.161\n",
      "[4,    60] loss: 3.158\n",
      "[4,    80] loss: 3.157\n",
      "[5,    20] loss: 3.148\n",
      "[5,    40] loss: 3.147\n",
      "[5,    60] loss: 3.144\n",
      "[5,    80] loss: 3.140\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.218\n",
      "[1,    40] loss: 3.213\n",
      "[1,    60] loss: 3.209\n",
      "[1,    80] loss: 3.204\n",
      "[2,    20] loss: 3.199\n",
      "[2,    40] loss: 3.194\n",
      "[2,    60] loss: 3.190\n",
      "[2,    80] loss: 3.187\n",
      "[3,    20] loss: 3.180\n",
      "[3,    40] loss: 3.178\n",
      "[3,    80] loss: 3.171\n",
      "[4,    20] loss: 3.163\n",
      "[4,    40] loss: 3.158\n",
      "[4,    60] loss: 3.159\n",
      "[4,    80] loss: 3.154\n",
      "[5,    20] loss: 3.148\n",
      "[5,    40] loss: 3.148\n",
      "[5,    60] loss: 3.144\n",
      "[5,    80] loss: 3.137\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[1,    40] loss: 3.213\n",
      "[1,    60] loss: 3.208\n",
      "[1,    80] loss: 3.205\n",
      "[2,    20] loss: 3.199\n",
      "[2,    40] loss: 3.194\n",
      "[2,    60] loss: 3.190\n",
      "[2,    80] loss: 3.186\n",
      "[3,    20] loss: 3.180\n",
      "[3,    40] loss: 3.177\n",
      "[3,    60] loss: 3.173\n",
      "[3,    80] loss: 3.167\n",
      "[4,    20] loss: 3.164\n",
      "[4,    40] loss: 3.156\n",
      "[4,    60] loss: 3.157\n",
      "[4,    80] loss: 3.155\n",
      "[5,    20] loss: 3.150\n",
      "[5,    40] loss: 3.146\n",
      "[5,    60] loss: 3.141\n",
      "[5,    80] loss: 3.140\n",
      "---ALL FOLD ACCURACIES FOR CURRENT LR/BATCH_SIZE---\n",
      "[18.348002708192283, 17.670954637779282, 17.40013540961408, 17.005420054200542, 17.479674796747968]\n",
      "Average = 17.580837521306833\n",
      "\n",
      "LEARNING RATE:  0.003\n",
      "BATCH SIZE:  128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[1,    40] loss: 3.213\n",
      "[2,    20] loss: 3.207\n",
      "[2,    40] loss: 3.203\n",
      "[3,    20] loss: 3.197\n",
      "[3,    40] loss: 3.194\n",
      "[4,    20] loss: 3.189\n",
      "[4,    40] loss: 3.184\n",
      "[5,    20] loss: 3.180\n",
      "[5,    40] loss: 3.177\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[1,    40] loss: 3.213\n",
      "[2,    20] loss: 3.208\n",
      "[2,    40] loss: 3.203\n",
      "[3,    20] loss: 3.197\n",
      "[3,    40] loss: 3.194\n",
      "[4,    20] loss: 3.188\n",
      "[4,    40] loss: 3.184\n",
      "[5,    20] loss: 3.180\n",
      "[5,    40] loss: 3.176\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[1,    40] loss: 3.212\n",
      "[2,    20] loss: 3.208\n",
      "[2,    40] loss: 3.203\n",
      "[3,    20] loss: 3.197\n",
      "[3,    40] loss: 3.194\n",
      "[4,    20] loss: 3.189\n",
      "[4,    40] loss: 3.184\n",
      "[5,    20] loss: 3.178\n",
      "[5,    40] loss: 3.178\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[1,    40] loss: 3.213\n",
      "[2,    20] loss: 3.208\n",
      "[2,    40] loss: 3.203\n",
      "[3,    20] loss: 3.197\n",
      "[3,    40] loss: 3.194\n",
      "[4,    20] loss: 3.189\n",
      "[4,    40] loss: 3.184\n",
      "[5,    20] loss: 3.180\n",
      "[5,    40] loss: 3.176\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[1,    40] loss: 3.213\n",
      "[2,    20] loss: 3.207\n",
      "[2,    40] loss: 3.203\n",
      "[3,    20] loss: 3.198\n",
      "[3,    40] loss: 3.193\n",
      "[4,    20] loss: 3.188\n",
      "[4,    40] loss: 3.185\n",
      "[5,    20] loss: 3.180\n",
      "[5,    40] loss: 3.174\n",
      "---ALL FOLD ACCURACIES FOR CURRENT LR/BATCH_SIZE---\n",
      "[18.280297901150984, 16.99390656736628, 17.670954637779282, 16.93766937669377, 17.479674796747968]\n",
      "Average = 17.47250065594766\n",
      "\n",
      "LEARNING RATE:  0.003\n",
      "BATCH SIZE:  256\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[2,    20] loss: 3.212\n",
      "[3,    20] loss: 3.207\n",
      "[4,    20] loss: 3.202\n",
      "[5,    20] loss: 3.197\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[2,    20] loss: 3.212\n",
      "[3,    20] loss: 3.207\n",
      "[4,    20] loss: 3.202\n",
      "[5,    20] loss: 3.197\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[2,    20] loss: 3.212\n",
      "[3,    20] loss: 3.207\n",
      "[4,    20] loss: 3.202\n",
      "[5,    20] loss: 3.197\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[2,    20] loss: 3.212\n",
      "[3,    20] loss: 3.207\n",
      "[4,    20] loss: 3.202\n",
      "[5,    20] loss: 3.197\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n",
      "[1,    20] loss: 3.217\n",
      "[2,    20] loss: 3.212\n",
      "[3,    20] loss: 3.207\n",
      "[4,    20] loss: 3.202\n",
      "[5,    20] loss: 3.197\n",
      "---ALL FOLD ACCURACIES FOR CURRENT LR/BATCH_SIZE---\n",
      "[16.858496953283684, 17.80636425186188, 16.858496953283684, 17.276422764227643, 16.666666666666664]\n",
      "Average = 17.09328951786471\n",
      "\n",
      "LEARNING RATE:  0.003\n",
      "BATCH SIZE:  512\n",
      "---STARTING NEW FOLD---\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "# perform stratified L-Fold Cross Validation testing to determine optimal learning rates and batch sizes for training\n",
    "\n",
    "L = 5\n",
    "num_epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rates = [0.003, 0.01, 0.03, 0.06] # from ViT paper\n",
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "\n",
    "optimal_lr, optimal_batch_size = perform_validation_testing(model=our_ViT, L=L, validationset=validationset, criterion=criterion, lr_array=learning_rates, batchsize_array=batch_sizes, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a168290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths to training and testing folders, and create loaders using optimal batch size\n",
    "\n",
    "trainpath = \"/projectnb/dl523/students/kjv/EC520_Project/Data/wikipaintings_full/wikipaintings_train\"\n",
    "train_set = datasets.ImageFolder(trainpath, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=optimal_batch_size, shuffle = True)\n",
    "\n",
    "# initialize test dataset\n",
    "\n",
    "testpath = \"/projectnb/dl523/students/kjv/EC520_Project/Data/wikipaintings_full/wikipaintings_test\"\n",
    "\n",
    "testset = datasets.ImageFolder(testpath, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=optimal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random training images to verify import worked\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "# show images\n",
    "def imshow(img):\n",
    "    img = img * our_ViT.default_cfg['std'][0] + our_ViT.default_cfg['mean'][0]  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# show labels\n",
    "print(\"Class: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call finetune_ViT on our_ViT using optimal learning rate\n",
    "optimizer = optim.SGD(filter(lambda layer: layer.requires_grad, our_ViT.parameters()), lr=optimal_lr, momentum = .9)\n",
    "\n",
    "# set ViT to initial untrained weights\n",
    "our_ViT.load_state_dict(torch.load(\"/projectnb/dl523/students/kjv/EC520_Project/ViT/Saved_Models/untrained_ViT.pth\"))\n",
    "\n",
    "# train the ViT\n",
    "finetune_ViT(our_ViT, trainloader=trainloader, optimizer=optimizer, criterion=criterion, num_epochs=num_epochs)\n",
    "\n",
    "# save final model\n",
    "torch.save(our_ViT.state_dict(), \"/projectnb/dl523/students/kjv/EC520_Project/ViT/Saved_Models/v1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3088279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test finetuned ViT\n",
    "acc = test_ViT(our_ViT, testloader)\n",
    "\n",
    "print(f\"Results: {acc}% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top categories per image (need to put into for loop if we want to use)\n",
    "\n",
    "#top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "#for i in range(top5_prob.size(0)):\n",
    "#    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c938ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35090a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
